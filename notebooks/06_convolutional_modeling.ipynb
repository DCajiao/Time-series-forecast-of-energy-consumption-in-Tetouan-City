{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCajiao/Time-series-forecast-of-energy-consumption-in-Tetouan-City/blob/main/notebooks/06_convolutional_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a607474",
      "metadata": {
        "id": "0a607474"
      },
      "source": [
        "### Entrenamiento, Predicción y Evaluación de un modelo LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98525a11",
      "metadata": {
        "id": "98525a11"
      },
      "source": [
        "#### **Importación de Datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "966b1c19",
      "metadata": {
        "id": "966b1c19"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys, os, math, typing as t\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from io import BytesIO\n",
        "from dataclasses import dataclass\n",
        "from dataclasses import dataclass\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from datetime import timedelta\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, mixed_precision\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": (12, 5),\n",
        "    \"axes.grid\": True\n",
        "})\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2438d989",
      "metadata": {
        "id": "2438d989"
      },
      "outputs": [],
      "source": [
        "# Importar datos\n",
        "DATA_GITHUB_URL = 'https://raw.githubusercontent.com/DCajiao/Time-series-forecast-of-energy-consumption-in-Tetouan-City/refs/heads/main/data/zone1_power_consumption_of_tetouan_city.csv'\n",
        "\n",
        "# Descargar los datos desde github\n",
        "response = requests.get(DATA_GITHUB_URL)\n",
        "\n",
        "# Convertir en un df desde el xlsx de github\n",
        "df = pd.read_csv(BytesIO(response.content), sep=',')\n",
        "\n",
        "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
        "df = df.set_index(\"datetime\")\n",
        "\n",
        "# Validaciones mínimas\n",
        "expected_cols = {\"temperature\",\"humidity\",\"general_diffuse_flows\",\"zone_1\"}\n",
        "missing = expected_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Faltan columnas en el dataset: {missing}. \"\n",
        "                     f\"Columnas disponibles: {df.columns.tolist()}\")\n",
        "\n",
        "print(\"\\nFrecuencia aproximada:\", (df.index.to_series().diff().mode().iloc[0]))\n",
        "print(\"Filas totales:\", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da0b5ade",
      "metadata": {
        "id": "da0b5ade"
      },
      "source": [
        "#### **Definición de Funciones y Partición de los datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f38c1225",
      "metadata": {
        "id": "f38c1225"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Dict\n",
        "\n",
        "def temporal_split(df: pd.DataFrame, train_size=0.7, val_size=0.2):\n",
        "    \"\"\"Split temporal 70/20/10 por defecto.\"\"\"\n",
        "    n = len(df)\n",
        "    n_train = int(n * train_size)\n",
        "    n_val = int(n * val_size)\n",
        "    train = df.iloc[:n_train]\n",
        "    val = df.iloc[n_train:n_train+n_val]\n",
        "    test = df.iloc[n_train+n_val:]\n",
        "    return train, val, test\n",
        "\n",
        "def make_windows(\n",
        "    data: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    feature_cols: t.List[str],\n",
        "    history: int = 1008,   # 7 días de 10-min\n",
        "    target_shift: int = 1   # 1 paso adelante\n",
        "):\n",
        "    \"\"\"Crea X, y con ventana deslizante. X tiene forma (N, history, F).\n",
        "    Para modelos tipo sklearn, se devuelve además X2 (aplanado).\"\"\"\n",
        "    feats = data[feature_cols].values\n",
        "    target = data[target_col].values\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - history - target_shift + 1):\n",
        "        X.append(feats[i:i+history])\n",
        "        y.append(target[i+history+target_shift-1])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    # Aplanado para modelos de tabular (árboles, etc.)\n",
        "    ns, h, f = X.shape\n",
        "    X2 = X.reshape((ns, h*f))\n",
        "    return X, X2, y\n",
        "\n",
        "def smape(y_true, y_pred, eps: float = 1e-8):\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) + eps\n",
        "    return np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0\n",
        "\n",
        "def wape(y_true, y_pred, eps: float = 1e-8):\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "    return (np.sum(np.abs(y_pred - y_true)) / (np.sum(np.abs(y_true)) + eps)) * 100.0\n",
        "\n",
        "@dataclass\n",
        "class Scalers:\n",
        "    X: Optional[StandardScaler] = None\n",
        "    y: Optional[StandardScaler] = None\n",
        "\n",
        "def fit_scalers(train_df: pd.DataFrame, feature_cols: t.List[str], target_col: str) -> Scalers:\n",
        "    sx = StandardScaler()\n",
        "    sy = StandardScaler()\n",
        "    sx.fit(train_df[feature_cols])\n",
        "    sy.fit(train_df[[target_col]])\n",
        "    return Scalers(X=sx, y=sy)\n",
        "\n",
        "def apply_scalers(df: pd.DataFrame, scalers: Scalers, feature_cols: t.List[str], target_col: str):\n",
        "    out = df.copy()\n",
        "    out[feature_cols] = scalers.X.transform(out[feature_cols])\n",
        "    out[target_col]  = scalers.y.transform(out[[target_col]])\n",
        "    return out\n",
        "\n",
        "def inverse_target(y: np.ndarray, scalers: Scalers) -> np.ndarray:\n",
        "    return scalers.y.inverse_transform(y.reshape(-1,1)).ravel()\n",
        "\n",
        "def plot_segment(idx, y_true, y_pred, title=\"Predicción (tramo)\", target_name=\"target\"):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.plot(idx, y_true, label=\"Real\", linewidth=2)\n",
        "    plt.plot(idx, y_pred, label=\"Pred\", linewidth=2)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Tiempo\"); plt.ylabel(target_name); plt.legend(); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1820226",
      "metadata": {
        "id": "c1820226"
      },
      "outputs": [],
      "source": [
        "TARGET_COL = \"zone_1\"  # consumo a pronosticar\n",
        "EXOG_COLS = [\"temperature\", \"humidity\", \"general_diffuse_flows\"]  # exógenas priorizadas\n",
        "\n",
        "# Ordenar columnas por claridad (target al final)\n",
        "cols = EXOG_COLS + [TARGET_COL]\n",
        "df2 = df[cols].copy().astype(\"float32\")\n",
        "\n",
        "# Splits temporales base (para val/test)\n",
        "train_df_full, val_df, test_df = temporal_split(df2, train_size=0.7, val_size=0.2)\n",
        "print({s: len(x) for s,x in [('train_full',train_df_full),('val',val_df),('test',test_df)]})\n",
        "\n",
        "\n",
        "# Entrenar con los últimos 45 días\n",
        "POINTS_PER_DAY = 24 * 6           # 10 min = 6 puntos por hora = 144 por día\n",
        "TRAIN_DAYS = 45\n",
        "WINDOW_TRAIN_POINTS = TRAIN_DAYS * POINTS_PER_DAY  # 45 * 144 = 6480\n",
        "\n",
        "HISTORY_WINDOW = 7 * POINTS_PER_DAY   # 7 días = 1008 pasos\n",
        "SHIFT_ONE_STEP = 1                    # +10 min\n",
        "\n",
        "# Tomamos exactamente los últimos 45 días como training\n",
        "train_df = train_df_full.tail(WINDOW_TRAIN_POINTS)\n",
        "\n",
        "print(\n",
        "    \"train_df (últimos 45 días) =\",\n",
        "    train_df.index.min(), \"→\", train_df.index.max(),\n",
        "    \"| filas:\", len(train_df)\n",
        ")\n",
        "\n",
        "\n",
        "# Escalado Z-score (stats SOLO del train de 45 días)\n",
        "scalers = fit_scalers(train_df, EXOG_COLS, TARGET_COL)\n",
        "train_n = apply_scalers(train_df, scalers, EXOG_COLS, TARGET_COL)\n",
        "val_n   = apply_scalers(val_df,   scalers, EXOG_COLS, TARGET_COL)\n",
        "test_n  = apply_scalers(test_df,  scalers, EXOG_COLS, TARGET_COL)\n",
        "\n",
        "# Ventanas one-step (history=1008, shift=1)\n",
        "Xtr, Xtr2, ytr = make_windows(\n",
        "    train_n, TARGET_COL, EXOG_COLS + [TARGET_COL],\n",
        "    history=HISTORY_WINDOW, target_shift=SHIFT_ONE_STEP\n",
        ")\n",
        "Xva, Xva2, yva = make_windows(\n",
        "    val_n, TARGET_COL, EXOG_COLS + [TARGET_COL],\n",
        "    history=HISTORY_WINDOW, target_shift=SHIFT_ONE_STEP\n",
        ")\n",
        "Xte, Xte2, yte = make_windows(\n",
        "    test_n, TARGET_COL, EXOG_COLS + [TARGET_COL],\n",
        "    history=HISTORY_WINDOW, target_shift=SHIFT_ONE_STEP\n",
        ")\n",
        "\n",
        "print(\"Shapes →\",\n",
        "      \"Xtr2:\", Xtr2.shape, \"ytr:\", ytr.shape, \"|\",\n",
        "      \"Xva2:\", Xva2.shape, \"yva:\", yva.shape, \"|\",\n",
        "      \"Xte2:\", Xte2.shape, \"yte:\", yte.shape)\n",
        "\n",
        "# Validación rápida\n",
        "assert Xtr2.shape[0] >= 1, \"No hay suficientes filas en los últimos 45 días para formar al menos 1 ventana.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec1c33a3",
      "metadata": {
        "id": "ec1c33a3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bf49f8",
      "metadata": {
        "id": "16bf49f8"
      },
      "source": [
        "#### **Entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff67599",
      "metadata": {
        "id": "1ff67599"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "def build_cnn1d(input_steps: int, n_features: int, out_dim: int = 1):\n",
        "    \"\"\"\n",
        "    CNN 1D causal para series:\n",
        "    - Conv1D captura patrones locales; padding='causal' preserva temporalidad.\n",
        "    - Bloques conv → BN → activación → pooling reducen varianza.\n",
        "    - GAP + Dense: resumen robusto + proyección al horizonte (1 paso).\n",
        "    \"\"\"\n",
        "    inp = keras.Input(shape=(input_steps, n_features), dtype=\"float32\")\n",
        "    x = layers.Conv1D(64, kernel_size=5, padding=\"causal\", activation=\"relu\")(inp)\n",
        "    x = layers.Conv1D(64, kernel_size=3, padding=\"causal\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    x = layers.Conv1D(128, kernel_size=3, padding=\"causal\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    x = layers.Dense(128, activation=\"relu\",\n",
        "                     kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    out = layers.Dense(out_dim, dtype=\"float32\")(x)\n",
        "\n",
        "    model = keras.Model(inp, out, name=\"cnn1d_one_step\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"mse\",\n",
        "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr, _, ytr = make_windows(\n",
        "    train_n, TARGET_COL, EXOG_COLS + [TARGET_COL],\n",
        "    history=HISTORY_WINDOW, target_shift=SHIFT_ONE_STEP\n",
        ")\n",
        "Xva, _, yva = make_windows(\n",
        "    val_n, TARGET_COL, EXOG_COLS + [TARGET_COL],\n",
        "    history=HISTORY_WINDOW, target_shift=SHIFT_ONE_STEP\n",
        ")\n",
        "Xte, _, yte = make_windows(\n",
        "    test_n, TARGET_COL, EXOG_COLS + [TARGET_COL],\n",
        "    history=HISTORY_WINDOW, target_shift=SHIFT_ONE_STEP\n",
        ")\n",
        "\n",
        "Xtr = Xtr.astype(\"float32\"); ytr = ytr.astype(\"float32\")\n",
        "Xva = Xva.astype(\"float32\"); yva = yva.astype(\"float32\")\n",
        "Xte = Xte.astype(\"float32\"); yte = yte.astype(\"float32\")\n",
        "\n",
        "Xtr.shape, Xva.shape, Xte.shape\n"
      ],
      "metadata": {
        "id": "qzzYbGdTVoSe"
      },
      "id": "qzzYbGdTVoSe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbs = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
        "                                  patience=20, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\",\n",
        "                                      factor=0.5, patience=8, min_lr=1e-5, verbose=1)\n",
        "]\n",
        "\n",
        "cnn = build_cnn1d(input_steps=Xtr.shape[1], n_features=Xtr.shape[2], out_dim=1)\n",
        "cnn.summary()\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "hist_cnn = cnn.fit(\n",
        "    Xtr, ytr,\n",
        "    validation_data=(Xva, yva),\n",
        "    epochs=400,\n",
        "    batch_size=256,\n",
        "    verbose=2,\n",
        "    callbacks=cbs\n",
        ")\n",
        "t_train_cnn = time.time() - start\n",
        "print(f\"Tiempo de entrenamiento CNN: {t_train_cnn:.2f} s\")\n"
      ],
      "metadata": {
        "id": "5fc4nwJ_VsH2"
      },
      "id": "5fc4nwJ_VsH2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "93ff4b2f",
      "metadata": {
        "id": "93ff4b2f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3edf57",
      "metadata": {
        "id": "ad3edf57"
      },
      "source": [
        "#### **Predicción**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0f9bf9",
      "metadata": {
        "id": "ad0f9bf9"
      },
      "source": [
        "A un paso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a958630",
      "metadata": {
        "id": "2a958630"
      },
      "outputs": [],
      "source": [
        "# Predicciones normalizadas\n",
        "y_pred_val_cnn = cnn.predict(Xva, batch_size=512, verbose=0).reshape(-1)\n",
        "y_pred_te_cnn  = cnn.predict(Xte, batch_size=512, verbose=0).reshape(-1)\n",
        "\n",
        "# Invertir escala a unidades originales\n",
        "y_val_inv_cnn      = inverse_target(yva, scalers)\n",
        "y_pred_val_inv_cnn = inverse_target(y_pred_val_cnn, scalers)\n",
        "y_te_inv_cnn       = inverse_target(yte, scalers)\n",
        "y_pred_te_inv_cnn  = inverse_target(y_pred_te_cnn, scalers)\n",
        "\n",
        "# Tramo de visualización en TEST (opcional, igual a tu LSTM)\n",
        "start = HISTORY_WINDOW + SHIFT_ONE_STEP - 1\n",
        "idx_seg = test_df.index[start : start + len(y_te_inv_cnn)]\n",
        "\n",
        "plot_segment(idx_seg[:800], y_te_inv_cnn[:800], y_pred_te_inv_cnn[:800],\n",
        "             title=\"CNN 1D — One-step (+10 min)\", target_name=TARGET_COL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2503a9",
      "metadata": {
        "id": "ec2503a9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e538244",
      "metadata": {
        "id": "3e538244"
      },
      "source": [
        "A múltiples pasos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85405aa",
      "metadata": {
        "id": "e85405aa"
      },
      "outputs": [],
      "source": [
        "# ==============================================\n",
        "# PRONÓSTICO A MÚLTIPLES PASOS — Recursive (3h)\n",
        "# ==============================================\n",
        "# Fundamento:\n",
        "# - Autoregresivo: usar la predicción y_hat(t+1) como nuevo último lag para predecir t+2, etc.\n",
        "# - Exógenas: por defecto estrategia HOLD (se mantienen fijas); si tienes pronósticos exógenos,\n",
        "#   pásalos normalizados para reducir degradación (exog_future).\n",
        "# - Para LSTM el input debe ser 3D: (1, HISTORY_WINDOW, F) en cada iteración.\n",
        "\n",
        "def recursive_forecast_seq(\n",
        "    model: keras.Model,\n",
        "    seed_window: np.ndarray,           # (HISTORY_WINDOW, F) normalizado; orden: EXOG_COLS + [TARGET_COL]\n",
        "    steps: int,\n",
        "    exog_future: np.ndarray | None = None  # opcional: (steps, len(EXOG_COLS)) en escala normalizada\n",
        ") -> np.ndarray:\n",
        "    hist = seed_window.astype(\"float32\").copy()\n",
        "    preds = []\n",
        "    for i in range(steps):\n",
        "        x = hist[np.newaxis, :, :]                 # (1, H, F)\n",
        "        y_hat = model.predict(x, verbose=0)[0, 0]  # escalar normalizado\n",
        "        preds.append(y_hat)\n",
        "        # Construimos la siguiente fila: exógenas hold o exog_future, target = y_hat\n",
        "        next_row = hist[-1].copy()\n",
        "        if exog_future is not None:\n",
        "            next_row[:-1] = exog_future[i]        # reemplaza TODAS las exógenas\n",
        "        # actualizar el target con la predicción\n",
        "        next_row[-1] = y_hat\n",
        "        # desplazar ventana\n",
        "        hist = np.vstack([hist[1:], next_row])\n",
        "    return np.array(preds, dtype=\"float32\")\n",
        "\n",
        "STEPS = 18  # 3 horas (18 * 10 min)\n",
        "feats_order = EXOG_COLS + [TARGET_COL]\n",
        "\n",
        "# Elegimos semilla en TEST dejando espacio para 'STEPS' futuros\n",
        "seed_start = max(0, len(test_n) - (HISTORY_WINDOW + STEPS))\n",
        "seed_end   = seed_start + HISTORY_WINDOW\n",
        "seed_win   = test_n[feats_order].values[seed_start:seed_end]  # (H, F) normalizado\n",
        "\n",
        "# Predicción recursiva (normalizada) y desnormalización\n",
        "y_pred_rec_norm_lstm = recursive_forecast_seq(lstm, seed_win, STEPS, exog_future=None)\n",
        "y_pred_rec_lstm      = inverse_target(y_pred_rec_norm_lstm, scalers)\n",
        "\n",
        "# Verdaderos y eje temporal para evaluación homogénea\n",
        "true_start = seed_end\n",
        "true_end   = seed_end + STEPS\n",
        "y_true_rec = test_df[TARGET_COL].values[true_start:true_end]\n",
        "idx_future = test_df.index[true_start:true_end]\n",
        "\n",
        "plot_segment(idx_future, y_true_rec, y_pred_rec_lstm,\n",
        "             title=\"LSTM — Recursive (+3h)\", target_name=TARGET_COL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8701fb",
      "metadata": {
        "id": "5b8701fb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a20895d",
      "metadata": {
        "id": "0a20895d"
      },
      "source": [
        "#### **Evaluación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8b78b26",
      "metadata": {
        "id": "c8b78b26"
      },
      "outputs": [],
      "source": [
        "# One-step\n",
        "smape_val_cnn = smape(y_val_inv_cnn, y_pred_val_inv_cnn)\n",
        "wape_val_cnn  = wape(y_val_inv_cnn, y_pred_val_inv_cnn)\n",
        "smape_te_cnn  = smape(y_te_inv_cnn,  y_pred_te_inv_cnn)\n",
        "wape_te_cnn   = wape(y_te_inv_cnn,   y_pred_te_inv_cnn)\n",
        "\n",
        "print(f\"[CNN] ONE-STEP → VAL  sMAPE: {smape_val_cnn:.3f}% | WAPE: {wape_val_cnn:.3f}%\")\n",
        "print(f\"[CNN] ONE-STEP → TEST sMAPE: {smape_te_cnn:.3f}% | WAPE: {wape_te_cnn:.3f}%\")\n",
        "\n",
        "# Recursive (+3h)\n",
        "smape_rec_cnn = smape(y_true_rec, y_pred_rec_cnn)\n",
        "wape_rec_cnn  = wape(y_true_rec, y_pred_rec_cnn)\n",
        "print(f\"[CNN] RECURSIVE (3h) → TEST sMAPE: {smape_rec_cnn:.3f}% | WAPE: {wape_rec_cnn:.3f}%\")\n",
        "print(f\"[CNN] Training time: {t_train_cnn:.2f} segundos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f97287",
      "metadata": {
        "id": "e4f97287"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11fce770",
      "metadata": {
        "id": "11fce770"
      },
      "source": [
        "#### Referencias:\n",
        "-"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
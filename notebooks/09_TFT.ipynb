{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1f1885",
   "metadata": {},
   "source": [
    "### Entrenamiento, Predicción y Evaluación de un modelo LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce61853",
   "metadata": {},
   "source": [
    "#### **Importación de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c79a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os, math, typing as t\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from io import BytesIO\n",
    "from dataclasses import dataclass\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import timedelta\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, mixed_precision\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (12, 5),\n",
    "    \"axes.grid\": True\n",
    "})\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "mixed_precision.set_global_policy(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ab8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar datos\n",
    "DATA_GITHUB_URL = 'https://raw.githubusercontent.com/DCajiao/Time-series-forecast-of-energy-consumption-in-Tetouan-City/refs/heads/main/data/zone1_power_consumption_of_tetouan_city.csv'\n",
    "\n",
    "# Descargar los datos desde github\n",
    "response = requests.get(DATA_GITHUB_URL)\n",
    "\n",
    "# Convertir en un df desde el xlsx de github\n",
    "df = pd.read_csv(BytesIO(response.content), sep=',')\n",
    "\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "df = df.set_index(\"datetime\")\n",
    "\n",
    "# Validaciones mínimas\n",
    "expected_cols = {\"temperature\",\"humidity\",\"general_diffuse_flows\",\"zone_1\"}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas en el dataset: {missing}. \"\n",
    "                     f\"Columnas disponibles: {df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nFrecuencia aproximada:\", (df.index.to_series().diff().mode().iloc[0]))\n",
    "print(\"Filas totales:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754b371",
   "metadata": {},
   "source": [
    "#### **Definición de Funciones y Partición de los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "\n",
    "def temporal_split(df: pd.DataFrame, train_size=0.7, val_size=0.2):\n",
    "    \"\"\"Split temporal 70/20/10 por defecto.\"\"\"\n",
    "    n = len(df)\n",
    "    n_train = int(n * train_size)\n",
    "    n_val = int(n * val_size)\n",
    "    train = df.iloc[:n_train]\n",
    "    val = df.iloc[n_train:n_train+n_val]\n",
    "    test = df.iloc[n_train+n_val:]\n",
    "    return train, val, test\n",
    "\n",
    "def make_windows(\n",
    "    data: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    feature_cols: t.List[str],\n",
    "    history: int = 1008,   # 7 días de 10-min\n",
    "    target_shift: int = 1   # 1 paso adelante\n",
    "):\n",
    "    \"\"\"Crea X, y con ventana deslizante. X tiene forma (N, history, F).\n",
    "    Para modelos tipo sklearn, se devuelve además X2 (aplanado).\"\"\"\n",
    "    feats = data[feature_cols].values\n",
    "    target = data[target_col].values\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - history - target_shift + 1):\n",
    "        X.append(feats[i:i+history])\n",
    "        y.append(target[i+history+target_shift-1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # Aplanado para modelos de tabular (árboles, etc.)\n",
    "    ns, h, f = X.shape\n",
    "    X2 = X.reshape((ns, h*f))\n",
    "    return X, X2, y\n",
    "\n",
    "def smape(y_true, y_pred, eps: float = 1e-8):\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) + eps\n",
    "    return np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0\n",
    "\n",
    "def wape(y_true, y_pred, eps: float = 1e-8):\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    return (np.sum(np.abs(y_pred - y_true)) / (np.sum(np.abs(y_true)) + eps)) * 100.0\n",
    "\n",
    "@dataclass\n",
    "class Scalers:\n",
    "    X: Optional[StandardScaler] = None\n",
    "    y: Optional[StandardScaler] = None\n",
    "\n",
    "def fit_scalers(train_df: pd.DataFrame, feature_cols: t.List[str], target_col: str) -> Scalers:\n",
    "    sx = StandardScaler()\n",
    "    sy = StandardScaler()\n",
    "    sx.fit(train_df[feature_cols])\n",
    "    sy.fit(train_df[[target_col]])\n",
    "    return Scalers(X=sx, y=sy)\n",
    "\n",
    "def apply_scalers(df: pd.DataFrame, scalers: Scalers, feature_cols: t.List[str], target_col: str):\n",
    "    out = df.copy()\n",
    "    out[feature_cols] = scalers.X.transform(out[feature_cols])\n",
    "    out[target_col]  = scalers.y.transform(out[[target_col]])\n",
    "    return out\n",
    "\n",
    "def inverse_target(y: np.ndarray, scalers: Scalers) -> np.ndarray:\n",
    "    return scalers.y.inverse_transform(y.reshape(-1,1)).ravel()\n",
    "\n",
    "def plot_segment(idx, y_true, y_pred, title=\"Predicción (tramo)\", target_name=\"target\"):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(idx, y_true, label=\"Real\", linewidth=2)\n",
    "    plt.plot(idx, y_pred, label=\"Pred\", linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Tiempo\"); plt.ylabel(target_name); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6fbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"zone_1\"  # consumo a pronosticar\n",
    "EXOG_COLS = [\"temperature\", \"humidity\", \"general_diffuse_flows\"]  # exógenas priorizadas\n",
    "\n",
    "# Ordenar columnas por claridad (target al final)\n",
    "cols = EXOG_COLS + [TARGET_COL]\n",
    "df2 = df[cols].copy().astype(\"float32\")\n",
    "\n",
    "# Splits temporales base (para val/test)\n",
    "train_df_full, val_df, test_df = temporal_split(df2, train_size=0.7, val_size=0.2)\n",
    "print({s: len(x) for s,x in [('train_full',train_df_full),('val',val_df),('test',test_df)]})\n",
    "\n",
    "\n",
    "# Entrenar con los últimos 45 días\n",
    "POINTS_PER_DAY = 24 * 6           # 10 min = 6 puntos por hora = 144 por día\n",
    "TRAIN_DAYS = 45\n",
    "WINDOW_TRAIN_POINTS = TRAIN_DAYS * POINTS_PER_DAY  # 45 * 144 = 6480\n",
    "\n",
    "HISTORY_WINDOW = 7 * POINTS_PER_DAY   # 7 días = 1008 pasos\n",
    "SHIFT_ONE_STEP = 1                    # +10 min\n",
    "\n",
    "# Tomamos exactamente los últimos 45 días como training\n",
    "train_df = train_df_full.tail(WINDOW_TRAIN_POINTS)\n",
    "\n",
    "print(\n",
    "    \"train_df (últimos 45 días) =\",\n",
    "    train_df.index.min(), \"→\", train_df.index.max(),\n",
    "    \"| filas:\", len(train_df)\n",
    ")\n",
    "\n",
    "\n",
    "# Escalado Z-score (stats SOLO del train de 45 días)\n",
    "scalers = fit_scalers(train_df, EXOG_COLS, TARGET_COL)\n",
    "train_n = apply_scalers(train_df, scalers, EXOG_COLS, TARGET_COL)\n",
    "val_n   = apply_scalers(val_df,   scalers, EXOG_COLS, TARGET_COL)\n",
    "test_n  = apply_scalers(test_df,  scalers, EXOG_COLS, TARGET_COL)\n",
    "\n",
    "# Ventanas one-step (history=1008, shift=1)\n",
    "Xtr, Xtr2, ytr = make_windows(\n",
    "    train_n, TARGET_COL, EXOG_COLS + [TARGET_COL],\n",
    "    history=HISTORY_WINDOW, target_shift=SHIFT_ONE_STEP\n",
    ")\n",
    "Xva, Xva2, yva = make_windows(\n",
    "    val_n, TARGET_COL, EXOG_COLS + [TARGET_COL],\n",
    "    history=HISTORY_WINDOW, target_shift=SHIFT_ONE_STEP\n",
    ")\n",
    "Xte, Xte2, yte = make_windows(\n",
    "    test_n, TARGET_COL, EXOG_COLS + [TARGET_COL],\n",
    "    history=HISTORY_WINDOW, target_shift=SHIFT_ONE_STEP\n",
    ")\n",
    "\n",
    "print(\"Shapes →\",\n",
    "      \"Xtr2:\", Xtr2.shape, \"ytr:\", ytr.shape, \"|\",\n",
    "      \"Xva2:\", Xva2.shape, \"yva:\", yva.shape, \"|\",\n",
    "      \"Xte2:\", Xte2.shape, \"yte:\", yte.shape)\n",
    "\n",
    "# Validación rápida\n",
    "assert Xtr2.shape[0] >= 1, \"No hay suficientes filas en los últimos 45 días para formar al menos 1 ventana.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cbdaaa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d45478",
   "metadata": {},
   "source": [
    "#### **Entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec007e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d29bb9f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176385d",
   "metadata": {},
   "source": [
    "#### **Predicción**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92114aab",
   "metadata": {},
   "source": [
    "A un paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f10cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
